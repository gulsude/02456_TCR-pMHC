{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18826ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import json, pickle\n",
    "from collections import OrderedDict\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "from utils import *\n",
    "\n",
    "def return_aa(one_hot):\n",
    "    mapping = dict(zip(range(20),\"ACDEFGHIKLMNPQRSTVWY\"))\n",
    "    try:\n",
    "        index = one_hot.index(1)\n",
    "        return mapping[index]     \n",
    "    except:\n",
    "        return 'X'\n",
    "\n",
    "def reverseOneHot(encoding):\n",
    "    \"\"\"\n",
    "    Converts one-hot encoded array back to string sequence\n",
    "    \"\"\"\n",
    "    seq=''\n",
    "    for i in range(len(encoding)):\n",
    "            if return_aa(encoding[i].tolist()) != 'X':\n",
    "                seq+=return_aa(encoding[i].tolist())\n",
    "    return seq\n",
    "\n",
    "def extract_sequences(dataset_X, merge=False):\n",
    "    \"\"\"\n",
    "    Return DataFrame with MHC, peptide and TCR a/b sequences from\n",
    "    one-hot encoded complex sequences in dataset X\n",
    "    \"\"\"\n",
    "    mhc_sequences = [reverseOneHot(arr[0:179,0:20]) for arr in dataset_X]\n",
    "    pep_sequences = [reverseOneHot(arr[179:192,0:20]) for arr in dataset_X] ## 190 or 192 ????\n",
    "    tcr_sequences = [reverseOneHot(arr[192:,0:20]) for arr in dataset_X]\n",
    "    all_sequences = [reverseOneHot(arr[179:192,0:20]) for arr in dataset_X]\n",
    "\n",
    "    if merge:\n",
    "        df_sequences = pd.DataFrame({\"all\": all_sequences})\n",
    "\n",
    "    else:\n",
    "        df_sequences = pd.DataFrame({\"MHC\":mhc_sequences,\n",
    "                                 \"peptide\":pep_sequences,\n",
    "                                 \"TCR\":tcr_sequences})\n",
    "        \n",
    "    return df_sequences    \n",
    "# nomarlize\n",
    "def dic_normalize(dic):\n",
    "    # print(dic)\n",
    "    max_value = dic[max(dic, key=dic.get)]\n",
    "    min_value = dic[min(dic, key=dic.get)]\n",
    "    # print(max_value)\n",
    "    interval = float(max_value) - float(min_value)\n",
    "    for key in dic.keys():\n",
    "        dic[key] = (dic[key] - min_value) / interval\n",
    "    dic['X'] = (max_value + min_value) / 2.0\n",
    "    return dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e4c5105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of file 0 0\n"
     ]
    }
   ],
   "source": [
    "data_list = []\n",
    "target_list = []\n",
    "\n",
    "import glob\n",
    "for fp in glob.glob(\"../data/train/*input.npz\"):\n",
    "    data = np.load(fp)[\"arr_0\"]\n",
    "    targets = np.load(fp.replace(\"input\", \"labels\"))[\"arr_0\"]\n",
    "    data_list.append(data)\n",
    "    target_list.append(targets)\n",
    "    \n",
    "for fp in glob.glob(\"../data/validation/*input.npz\"):\n",
    "    data = np.load(fp)[\"arr_0\"]\n",
    "    targets = np.load(fp.replace(\"input\", \"labels\"))[\"arr_0\"]\n",
    "    data_list.append(data)\n",
    "    target_list.append(targets)\n",
    "    \n",
    "#data_partitions = len(data_list[:-1])\n",
    "\n",
    "#print(\"Number of files:\", data_partitions)\n",
    "\n",
    "#for i in range(5):\n",
    "print(\"Size of file\", i, len(target_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a394b85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'GLCTLVAML'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def produced_key(n):\n",
    "    seq_key=[]\n",
    "    for i in range(n):\n",
    "        seq_key.append(i)\n",
    "    return seq_key\n",
    "\n",
    "seq_keys=[]\n",
    "seq_keys.append(produced_key(1480))\n",
    "seq_keys.append(produced_key(1532))\n",
    "seq_keys.append(produced_key(1168))\n",
    "seq_keys.append(produced_key(1526))\n",
    "seq_keys.append(produced_key(1207))\n",
    "print(seq_keys[0][0])\n",
    "seq_lists=[]\n",
    "for n in range(5):\n",
    "    m = n+1\n",
    "    seq_dir = os.path.join('GNN_data','data',str(m),'seq')\n",
    "    seq_list=[]\n",
    "    for i in range(len(seq_keys[n])):\n",
    "        seq_file = os.path.join(seq_dir, str(seq_keys[n][i])+ '.fasta')\n",
    "        infile = open(seq_file)\n",
    "        for line in infile:\n",
    "            if line.startswith('>'):\n",
    "                pass\n",
    "            else: \n",
    "                seq_list.append(line.strip())\n",
    "        #seq_list.append(infile.read()[3:-1])\n",
    "    seq_lists.append(seq_list)\n",
    "        \n",
    "seq_lists[0][1220]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1c2df583",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-7590fdf9dfcc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtarget_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a34c066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'G'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_lists[0][0][0][0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6976843a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_res_table = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y',\n",
    "                 'X']\n",
    "\n",
    "pro_res_aliphatic_table = ['A', 'I', 'L', 'M', 'V']\n",
    "pro_res_aromatic_table = ['F', 'W', 'Y']\n",
    "pro_res_polar_neutral_table = ['C', 'N', 'Q', 'S', 'T']\n",
    "pro_res_acidic_charged_table = ['D', 'E']\n",
    "pro_res_basic_charged_table = ['H', 'K', 'R']\n",
    "\n",
    "res_weight_table = {'A': 71.08, 'C': 103.15, 'D': 115.09, 'E': 129.12, 'F': 147.18, 'G': 57.05, 'H': 137.14,\n",
    "                    'I': 113.16, 'K': 128.18, 'L': 113.16, 'M': 131.20, 'N': 114.11, 'P': 97.12, 'Q': 128.13,\n",
    "                    'R': 156.19, 'S': 87.08, 'T': 101.11, 'V': 99.13, 'W': 186.22, 'Y': 163.18}\n",
    "\n",
    "res_pka_table = {'A': 2.34, 'C': 1.96, 'D': 1.88, 'E': 2.19, 'F': 1.83, 'G': 2.34, 'H': 1.82, 'I': 2.36,\n",
    "                 'K': 2.18, 'L': 2.36, 'M': 2.28, 'N': 2.02, 'P': 1.99, 'Q': 2.17, 'R': 2.17, 'S': 2.21,\n",
    "                 'T': 2.09, 'V': 2.32, 'W': 2.83, 'Y': 2.32}\n",
    "\n",
    "res_pkb_table = {'A': 9.69, 'C': 10.28, 'D': 9.60, 'E': 9.67, 'F': 9.13, 'G': 9.60, 'H': 9.17,\n",
    "                 'I': 9.60, 'K': 8.95, 'L': 9.60, 'M': 9.21, 'N': 8.80, 'P': 10.60, 'Q': 9.13,\n",
    "                 'R': 9.04, 'S': 9.15, 'T': 9.10, 'V': 9.62, 'W': 9.39, 'Y': 9.62}\n",
    "\n",
    "res_pkx_table = {'A': 0.00, 'C': 8.18, 'D': 3.65, 'E': 4.25, 'F': 0.00, 'G': 0, 'H': 6.00,\n",
    "                 'I': 0.00, 'K': 10.53, 'L': 0.00, 'M': 0.00, 'N': 0.00, 'P': 0.00, 'Q': 0.00,\n",
    "                 'R': 12.48, 'S': 0.00, 'T': 0.00, 'V': 0.00, 'W': 0.00, 'Y': 0.00}\n",
    "\n",
    "res_pl_table = {'A': 6.00, 'C': 5.07, 'D': 2.77, 'E': 3.22, 'F': 5.48, 'G': 5.97, 'H': 7.59,\n",
    "                'I': 6.02, 'K': 9.74, 'L': 5.98, 'M': 5.74, 'N': 5.41, 'P': 6.30, 'Q': 5.65,\n",
    "                'R': 10.76, 'S': 5.68, 'T': 5.60, 'V': 5.96, 'W': 5.89, 'Y': 5.96}\n",
    "\n",
    "res_hydrophobic_ph2_table = {'A': 47, 'C': 52, 'D': -18, 'E': 8, 'F': 92, 'G': 0, 'H': -42, 'I': 100,\n",
    "                             'K': -37, 'L': 100, 'M': 74, 'N': -41, 'P': -46, 'Q': -18, 'R': -26, 'S': -7,\n",
    "                             'T': 13, 'V': 79, 'W': 84, 'Y': 49}\n",
    "res_hydrophobic_ph7_table = {'A': 41, 'C': 49, 'D': -55, 'E': -31, 'F': 100, 'G': 0, 'H': 8, 'I': 99,\n",
    "                             'K': -23, 'L': 97, 'M': 74, 'N': -28, 'P': -46, 'Q': -10, 'R': -14, 'S': -5,\n",
    "                             'T': 13, 'V': 76, 'W': 97, 'Y': 63}\n",
    "\n",
    "res_weight_table = dic_normalize(res_weight_table)\n",
    "res_pka_table = dic_normalize(res_pka_table)\n",
    "res_pkb_table = dic_normalize(res_pkb_table)\n",
    "res_pkx_table = dic_normalize(res_pkx_table)\n",
    "res_pl_table = dic_normalize(res_pl_table)\n",
    "res_hydrophobic_ph2_table = dic_normalize(res_hydrophobic_ph2_table)\n",
    "res_hydrophobic_ph7_table = dic_normalize(res_hydrophobic_ph7_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75b68156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one ont encoding\n",
    "def one_of_k_encoding(x, allowable_set):\n",
    "    if x not in allowable_set:\n",
    "        # print(x)\n",
    "        raise Exception('input {0} not in allowable set{1}:'.format(x, allowable_set))\n",
    "    return list(map(lambda s: x == s, allowable_set))\n",
    "\n",
    "\n",
    "def one_of_k_encoding_unk(x, allowable_set):\n",
    "    '''Maps inputs not in the allowable set to the last element.'''\n",
    "    if x not in allowable_set:\n",
    "        x = allowable_set[-1]\n",
    "    return list(map(lambda s: x == s, allowable_set))\n",
    "\n",
    "def residue_features(residue):\n",
    "    res_property1 = [1 if residue in pro_res_aliphatic_table else 0, 1 if residue in pro_res_aromatic_table else 0,\n",
    "                     1 if residue in pro_res_polar_neutral_table else 0,\n",
    "                     1 if residue in pro_res_acidic_charged_table else 0,\n",
    "                     1 if residue in pro_res_basic_charged_table else 0]\n",
    "    res_property2 = [res_weight_table[residue], res_pka_table[residue], res_pkb_table[residue], res_pkx_table[residue],\n",
    "                     res_pl_table[residue], res_hydrophobic_ph2_table[residue], res_hydrophobic_ph7_table[residue]]\n",
    "    # print(np.array(res_property1 + res_property2).shape)\n",
    "    return np.array(res_property1 + res_property2)\n",
    "\n",
    "\n",
    "# target feature for target graph\n",
    "def PSSM_calculation(aln_file, pro_seq):\n",
    "    pfm_mat = np.zeros((len(pro_res_table), len(pro_seq)))\n",
    "    with open(aln_file, 'r') as f:\n",
    "        line_count = len(f.readlines())\n",
    "        for line in f.readlines():\n",
    "            if len(line) != len(pro_seq):\n",
    "                print('error', len(line), len(pro_seq))\n",
    "                continue\n",
    "            count = 0\n",
    "            for res in line:\n",
    "                if res not in pro_res_table:\n",
    "                    count += 1\n",
    "                    continue\n",
    "                pfm_mat[pro_res_table.index(res), count] += 1\n",
    "                count += 1\n",
    "    # ppm_mat = pfm_mat / float(line_count)\n",
    "    pseudocount = 0.8\n",
    "    ppm_mat = (pfm_mat + pseudocount / 4) / (float(line_count) + pseudocount)\n",
    "    pssm_mat = ppm_mat\n",
    "    # k = float(len(pro_res_table))\n",
    "    # pwm_mat = np.log2(ppm_mat / (1.0 / k))\n",
    "    # pssm_mat = pwm_mat\n",
    "    # print(pssm_mat)\n",
    "    return pssm_mat\n",
    "\n",
    "def seq_feature(pro_seq):\n",
    "    pro_hot = np.zeros((len(pro_seq), len(pro_res_table)))\n",
    "    pro_property = np.zeros((len(pro_seq), 12))\n",
    "    for i in range(len(pro_seq)):\n",
    "        # if 'X' in pro_seq:\n",
    "        #     print(pro_seq)\n",
    "        pro_hot[i,] = one_of_k_encoding(pro_seq[i], pro_res_table)\n",
    "        pro_property[i,] = residue_features(pro_seq[i])\n",
    "    return np.concatenate((pro_hot, pro_property), axis=1)\n",
    "\n",
    "\n",
    "def target_feature(aln_file, pro_seq):\n",
    "    pssm = PSSM_calculation(aln_file, pro_seq)\n",
    "    other_feature = seq_feature(pro_seq)\n",
    "    # print('target_feature')\n",
    "    # print(pssm.shape)\n",
    "    # print(other_feature.shape)\n",
    "\n",
    "    # print(other_feature.shape)\n",
    "    # return other_feature\n",
    "    return np.concatenate((np.transpose(pssm, (1, 0)), other_feature), axis=1)\n",
    "\n",
    "# target aln file save in data/dataset/aln\n",
    "def target_to_feature(target_key, target_sequence, aln_dir):\n",
    "    # aln_dir = 'data/' + dataset + '/aln'\n",
    "    aln_file = os.path.join(aln_dir, target_key + '.aln')\n",
    "    # if 'X' in target_sequence:\n",
    "    #     print(target_key)\n",
    "    feature = target_feature(aln_file, target_sequence)\n",
    "    return feature\n",
    "\n",
    "# pconsc4 predicted contact map save in data/dataset/pconsc4\n",
    "def target_to_graph(target_key, target_sequence, contact_dir, aln_dir):\n",
    "    target_edge_index = []\n",
    "    target_size = len(target_sequence)\n",
    "    # contact_dir = 'data/' + dataset + '/pconsc4'\n",
    "    contact_file = os.path.join(contact_dir, target_key + '.npy')\n",
    "    contact_map = np.load(contact_file)\n",
    "    contact_map += np.matrix(np.eye(contact_map.shape[0]))\n",
    "    index_row, index_col = np.where(contact_map >= 0.5)\n",
    "    for i, j in zip(index_row, index_col):\n",
    "        target_edge_index.append([i, j])\n",
    "    target_feature = target_to_feature(target_key, target_sequence, aln_dir)\n",
    "    target_edge_index = np.array(target_edge_index)\n",
    "    return target_size, target_feature, target_edge_index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7b5c625",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_graphs=[]\n",
    "for n in range(5):\n",
    "    m = n+1\n",
    "    aln_dir = os.path.join('GNN_data','data',str(m),'aln')\n",
    "    pconsc4_dir = os.path.join('GNN_data','data',str(m), 'pconsc4')\n",
    "    seq_graph = []\n",
    "    for i in range(len(seq_keys[n])):\n",
    "        g = target_to_graph(str(seq_keys[n][i]),seq_lists[n][i],pconsc4_dir,aln_dir)\n",
    "        \n",
    "        seq_graph.append(g)\n",
    "    seq_graphs.append(seq_graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "48b3840f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 54)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_graphs[0][-37][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f45fd160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "need at least one array to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-72a7f2870e48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mX_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_graphs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: need at least one array to concatenate"
     ]
    }
   ],
   "source": [
    "X_train = np.concatenate(seq_graphs[0:3])\n",
    "print(X_train[0][2][0])\n",
    "\n",
    "y_train = np.concatenate(target_list[0:3])\n",
    "print(X_train[0][0])\n",
    "X_valid = np.concatenate(seq_graphs[3:4])\n",
    "y_valid = np.concatenate(target_list[3:4])\n",
    "\n",
    "X_test = np.concatenate(seq_graphs[4:])\n",
    "y_test = np.concatenate(target_list[4:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "bedb1f32",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch_geometric'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-49a7dbe39416>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInMemoryDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mDATA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdata_proccess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdata_list_pro\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch_geometric'"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import InMemoryDataset, DataLoader, Batch\n",
    "from torch_geometric import data as DATA\n",
    "def data_proccess(graph_data,y):\n",
    "    data_list_pro = []\n",
    "    for i in range(len(graph_data)):\n",
    "        GCNData_pro = DATA.Data(x=torch.Tensor(graph_data[i][1]),\n",
    "                                    edge_index=torch.LongTensor(graph_data[i][2]).transpose(1, 0),\n",
    "                                    y=torch.FloatTensor([y[i]]))\n",
    "        GCNData_pro.__setitem__('target_size', torch.LongTensor([graph_data[i][0]]))\n",
    "            \n",
    "            \n",
    "        data_list_pro.append(GCNData_pro)\n",
    "    \n",
    "    \n",
    "    \n",
    "    data_pro = data_list_pro\n",
    "    \n",
    "    loader = torch.utils.data.DataLoader(data_pro, batch_size=TEST_BATCH_SIZE, shuffle=False,\n",
    "                                              collate_fn=collate)\n",
    "\n",
    "    \n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "29fdc803",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DATA' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-114-22fd4c7af930>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_proccess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-109-cab357faae12>\u001b[0m in \u001b[0;36mdata_proccess\u001b[0;34m(graph_data, y)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mdata_list_pro\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         GCNData_pro = DATA.Data(x=torch.Tensor(graph_data[i][1]),\n\u001b[0m\u001b[1;32m      5\u001b[0m                                     \u001b[0medge_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                     y=torch.FloatTensor([y[i]]))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DATA' is not defined"
     ]
    }
   ],
   "source": [
    "test = data_proccess(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f4d6543d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GCN + GRN\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.metrics import roc_curve, confusion_matrix\n",
    "import torch\n",
    "import torch.nn as nn  # All neural network modules, nn.Linear, nn.Conv2d, BatchNorm, Loss functions\n",
    "import torch.optim as optim  # For all Optimization algorithms, SGD, Adam, etc.\n",
    "import torch.nn.functional as F  # All functions that donâ€™t have any parameters\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0106435a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_val = 1\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "torch.set_deterministic(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b0e88111",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_thesis(nn.Module):\n",
    "    def __init__(self,  num_classes):\n",
    "        super(Net_thesis, self).__init__()\n",
    "        self.bn0 = nn.BatchNorm1d(n_features)\n",
    "        self.conv1 = nn.Conv1d(in_channels=n_features, out_channels=100, kernel_size=3, stride=2, padding=1)\n",
    "        torch.nn.init.kaiming_uniform_(self.conv1.weight)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.conv1_bn = nn.BatchNorm1d(100)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(in_channels=100, out_channels=100, kernel_size=3, stride=2, padding=1)\n",
    "        torch.nn.init.kaiming_uniform_(self.conv2.weight)\n",
    "        self.conv2_bn = nn.BatchNorm1d(100)\n",
    "\n",
    "        self.rnn = nn.LSTM(input_size=100,hidden_size=26,num_layers=3, dropout=0.1, batch_first=True, bidirectional = True)\n",
    "        self.drop = nn.Dropout(p = 0.1)\n",
    "\n",
    "        self.fc1 = nn.Linear(26*2, num_classes)\n",
    "        torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bn0(x)\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.conv1_bn(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.conv2_bn(x)\n",
    "        x = self.drop(x)\n",
    "        x = x.transpose_(2, 1)\n",
    "        x, (h, c) = self.rnn(x)\n",
    "        cat = torch.cat((h[-2, :, :], h[-1, :, :]), dim=1)\n",
    "        cat = self.drop(cat)\n",
    "        x = self.fc1(cat)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0badc0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "numHN=32\n",
    "numFilter=100\n",
    "dropOutRate=0.1\n",
    "\n",
    "n_features = 3\n",
    "input_size = len()\n",
    "##--- parameters fixed\n",
    "keep_energy=True\n",
    "cross_validation = False\n",
    "bat_size = 128\n",
    "num_classes=1\n",
    "learning_rate=0.001\n",
    "epochs = 100\n",
    "patience=10\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43c9a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------- Train --------#\n",
    "\n",
    "    \n",
    "    \n",
    "    # Initialize network\n",
    "    net = Net_project(num_classes=num_classes, \n",
    "             n_features=n_features, \n",
    "             numHN=numHN, \n",
    "             numFilter=numFilter,\n",
    "             dropOutRate=dropOutRate).to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate,\n",
    "                           weight_decay=0.0005,\n",
    "                           amsgrad=True,)\n",
    "    \n",
    "    train_acc, train_losses, train_auc, valid_acc, valid_losses, valid_auc, val_preds, val_targs, test_preds, test_targs, test_loss, test_acc, test_auc = func.train_project(net, optimizer, train_ldr, val_ldr, test_ldr, X_valid, epochs, criterion, patience)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
